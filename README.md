# Desafio MBA Engenharia de Software com IA - Full Cycle

Sistema de **RAG (Retrieval-Augmented Generation)** que permite fazer perguntas sobre um PDF usando LangChain, Google Gemini e PostgreSQL com pgVector.

## ğŸ¯ O que esta aplicaÃ§Ã£o faz?

Esta aplicaÃ§Ã£o implementa um sistema de chat inteligente baseado em RAG que permite fazer perguntas sobre documentos PDF:

1. **ğŸ“„ IngestÃ£o de PDF**: Carrega e processa documentos PDF, dividindo-os em chunks menores
2. **ğŸ§  VetorizaÃ§Ã£o**: Gera embeddings (representaÃ§Ãµes vetoriais) do conteÃºdo usando Google Gemini
3. **ğŸ’¾ Armazenamento**: Salva os vetores no PostgreSQL com pgVector para busca eficiente
4. **ğŸ” Busca SemÃ¢ntica**: Encontra os trechos mais relevantes do documento baseado no significado da pergunta
5. **ğŸ¤– GeraÃ§Ã£o de Resposta**: Usa o contexto encontrado para gerar respostas precisas via LLM (Gemini)
6. **âœ… ValidaÃ§Ã£o de Contexto**: Responde apenas com base no conteÃºdo do PDF, evitando alucinaÃ§Ãµes

## ğŸ“‹ Requisitos

- Python 3.10+
- Docker & Docker Compose
- Conta Google Cloud com API Key do Gemini

## ğŸš€ Como Executar

### 1. Clone o repositÃ³rio e navegue atÃ© a pasta do projeto

```bash
git clone <url-do-repositorio>
cd mba-ia-desafio-ingestao-busca
```

### 2. Configure as variÃ¡veis de ambiente

Copie o arquivo `.env.example` para `.env`:

```bash
cp .env.example .env
```

Edite o arquivo `.env` e configure sua **API Key do Google Gemini**:

```env
# Google Gemini API Configuration
GOOGLE_API_KEY=sua_chave_api_aqui

# Demais configuraÃ§Ãµes jÃ¡ estÃ£o prontas para uso
```

**Como obter a API Key do Google Gemini:**

1. Acesse: <https://aistudio.google.com/app/apikey>
2. Clique em "Create API Key"
3. Copie a chave gerada e cole no arquivo `.env`

### 3. Crie e ative um ambiente virtual Python

```bash
# Criar ambiente virtual
python -m venv .venv

# Ativar ambiente virtual
source .venv/bin/activate

```

### 4. Instale as dependÃªncias Python

```bash
pip install -r requirements.txt
```

### 5. Suba o banco de dados PostgreSQL com pgVector

```bash
docker-compose up -d
```

Aguarde o banco inicializar completamente (~10 segundos).

### 4. Ingerir o PDF no banco de dados

Coloque seu arquivo PDF na raiz do projeto (ou ajuste a variÃ¡vel `PDF_PATH` no `.env`).

Execute o script de ingestÃ£o:

```bash
python src/ingest.py
```

**SaÃ­da esperada:**

```bash
============================================================
INICIANDO PROCESSO DE INGESTÃƒO
============================================================
PDF: /caminho/para/document.pdf
Chunk Size: 1000
Chunk Overlap: 150
Embedding Model: models/embedding-001
============================================================

[1/4] Carregando PDF...
âœ“ PDF carregado com sucesso: 5 pÃ¡gina(s)

[2/4] Dividindo documento em chunks...
âœ“ Documento dividido em 42 chunk(s)

[3/4] Preparando documentos para ingestÃ£o...
âœ“ 42 documento(s) preparado(s)

[4/4] Gerando embeddings e salvando no PostgreSQL...
âœ“ 42 chunk(s) armazenado(s) com sucesso no banco de dados!

============================================================
INGESTÃƒO CONCLUÃDA COM SUCESSO!
============================================================

Agora vocÃª pode executar o chat.py para fazer perguntas sobre o PDF.
```

### 5. Executar o chat interativo

```bash
python src/chat.py
```

## ğŸ’¬ Exemplos de Uso

**Pergunta dentro do contexto do PDF:**

```bash
FaÃ§a sua pergunta:

PERGUNTA: Qual o faturamento da empresa Alfa IA IndÃºstria ?

Processando sua pergunta... âœ“

RESPOSTA: R$ 548.789.613,65

------------------------------------------------------------
```

**Pergunta fora do contexto:**

```bash
FaÃ§a sua pergunta:

PERGUNTA: Qual a capital do Brasil?

Processando sua pergunta... âœ“

RESPOSTA: NÃ£o tenho informaÃ§Ãµes necessÃ¡rias para responder sua pergunta.

------------------------------------------------------------
```

**Para sair do chat:**

```bash
FaÃ§a sua pergunta:

PERGUNTA: sair

Encerrando chat. AtÃ© logo! ğŸ‘‹
```

## ğŸ—ï¸ Arquitetura

### SeparaÃ§Ã£o de Responsabilidades

O projeto estÃ¡ organizado com clara separaÃ§Ã£o de responsabilidades:

- **`search.py`**: ContÃ©m toda a lÃ³gica de busca vetorial e RAG com LLM
  - ConexÃ£o com pgVector
  - Busca de documentos similares
  - ConfiguraÃ§Ã£o da chain RAG (prompt + LLM)
  - FunÃ§Ã£o `ask_question()` que executa o fluxo RAG completo
  - Pode ser executado standalone para testes

- **`chat.py`**: Interface CLI para interaÃ§Ã£o com o usuÃ¡rio
  - Loop interativo de perguntas/respostas
  - ValidaÃ§Ã£o de entrada
  - Chamada para `ask_question()` de `search.py`
  - FormataÃ§Ã£o da saÃ­da

- **`ingest.py`**: Script de ingestÃ£o do PDF
  - Carregamento do PDF
  - DivisÃ£o em chunks
  - GeraÃ§Ã£o de embeddings
  - Armazenamento no pgVector

### Fluxo de IngestÃ£o (`ingest.py`)

1. **Carregamento**: LÃª o PDF usando `PyPDFLoader`
2. **Split**: Divide em chunks de 1000 caracteres com overlap de 150
3. **Embedding**: Gera vetores usando `models/embedding-001` do Gemini
4. **Storage**: Armazena no PostgreSQL com pgVector

### Fluxo de Consulta (`chat.py` â†’ `search.py`)

1. **Input**: `chat.py` recebe pergunta do usuÃ¡rio via CLI
2. **DelegaÃ§Ã£o**: Chama `ask_question()` de `search.py`
3. **VetorizaÃ§Ã£o**: `search.py` converte pergunta em embedding
4. **Search**: Busca top-10 chunks mais similares (k=10)
5. **Context**: Concatena os chunks encontrados
6. **Prompt**: Monta prompt com contexto + regras + pergunta
7. **LLM**: Chama `gemini-2.5-flash-lite` para gerar resposta
8. **Output**: `chat.py` exibe resposta formatada ao usuÃ¡rio

## ğŸ“¦ Tecnologias Utilizadas

- **Python 3.10+**
- **LangChain** - Framework para aplicaÃ§Ãµes com LLMs
- **Google Gemini** - Modelo de embeddings e LLM
- **PostgreSQL 17** - Banco de dados relacional
- **pgVector** - ExtensÃ£o para busca vetorial
- **Docker** - ContainerizaÃ§Ã£o do banco de dados

## ğŸ“ Estrutura do Projeto

```
â”œâ”€â”€ document.pdf             # PDF para anÃ¡lise
â”œâ”€â”€ docker-compose.yml       # ConfiguraÃ§Ã£o do PostgreSQL + pgVector
â”œâ”€â”€ .env                     # VariÃ¡veis de ambiente (nÃ£o commitado)
â”œâ”€â”€ .env.example             # Template de configuraÃ§Ã£o
â”œâ”€â”€ requirements.txt         # DependÃªncias Python
â”œâ”€â”€ README.md                # DocumentaÃ§Ã£o do projeto
â””â”€â”€ src/ 
    â”œâ”€â”€ ingest.py            # Script de ingestÃ£o do PDF
    â”œâ”€â”€ search.py            # LÃ³gica de busca vetorial e RAG
    â””â”€â”€ chat.py              # Interface CLI interativa
```

## ğŸ“ Notas

- O modelo `gemini-2.5-flash-lite` Ã© usado com `temperature=0.0` para respostas mais determinÃ­sticas
- O sistema busca os 10 chunks mais relevantes (k=10) para cada pergunta
- O prompt forÃ§a a LLM a responder apenas com base no contexto fornecido
- Perguntas fora do escopo retornam: "NÃ£o tenho informaÃ§Ãµes necessÃ¡rias para responder sua pergunta."

Este projeto foi desenvolvido para o MBA de Engenharia de Software com IA - Full Cycle.
